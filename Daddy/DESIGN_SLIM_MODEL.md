
# DESIGN_SLIM_MODEL.md

# **Slim RL Agent Design Document (Daddy/)**  
### *A Clean, Efficient Replacement for the Old Model in `Final/epsilon/`*  
### *Fully Compatible with `pokemonred_puffer`*

---

## **â— Important Rules**

### **Do NOT modify any code in:**
- `pokemonred_puffer/` (official puffer environment + wrappers)
- `Final/epsilon/` (my old custom agent / baseline)

These must remain untouched and fully operational.

### **ALL new code must go inside:**
```
Daddy/
```

This will be the **entire codebase for the new slim model**.

---

# **ğŸ“ Directory Structure (Required)**

Create this structure under `Daddy/`:

```
Daddy/
  __init__.py

  agent.py                # Main SlimDQN agent class
  networks.py             # CNN + GRU/LSTM + Q-head
  rnd.py                  # Random Network Distillation
  flags.py                # WRAM flag decoding + embedding + objectives
  bayes_quests.py         # Bayesian posterior over quests + flag-likelihoods
  curriculum.py           # Multi-city savestate curriculum (swarming)
  replay_buffer.py        # Uniform or prioritized replay buffer

  logging_utils.py        # steps/sec, parallelization, flag logs, quest logs
  video_utils.py          # mp4 / gif rollout capture

  train_slim.py           # Main training loop using puffer envs
```

---

# **ğŸ¯ Objective**

Build a **smaller**, **clean**, **robust**, **value-based off-policy DQN agent** that:

### âœ”ï¸ Uses `pokemonred_puffer` for environments  
### âœ”ï¸ Works with vectorized envs (multi-env swarming)  
### âœ”ï¸ Reads WRAM flag state for objectives + features  
### âœ”ï¸ Integrates:
- RND intrinsic rewards  
- Episodic visit-count bonuses  
- Bayesian quest posterior + WRAM flag likelihood modeling  
- Curriculum learning with savestates  
- Full logging (steps/sec, parallelization stats, flag likelihoods, quest posteriors)  
- Video generation (episode rollouts)  

---

# **ğŸ§  Model Requirements (SlimDQN)**

### Input:
- Stacked grayscale frames (72Ã—80, 4â€“8 frames)
- Structured features (map coords, city id, quest indicators)
- **WRAM flag embeddings** (decoded + compact)

### Architecture:
- **Slim CNN**: 2â€“3 conv layers, small channel sizes  
- **Recurrent head**: GRU or LSTM, ~128 hidden size  
- **Q-head**: linear output for discrete button actions  
- Total params ideally **2â€“6M**

### Forward API:
```
forward(obs, structured_features, hidden_state, done_mask)
  â†’ (q_values, new_hidden_state, aux_outputs)
```

---

# **âš™ï¸ Training Loop Requirements (`train_slim.py`)**

### 1. **Vectorized environments**
- Multi-env rollout (â€œswarmingâ€)
- Each env wrapped by `EpsilonEnv`
- Curriculum chooses savestates per-env

### 2. **DQN core**
- Off-policy learning  
- Replay buffer  
- Target network with periodic sync  
- Îµ-greedy schedule  

### 3. **Intrinsic motivation**
- RND prediction error reward  
- Episodic novel state bonus  

### 4. **Bayesian quest + flag posterior**
- Maintain posterior over quests/cities  
- Use WRAM flag observations  
- Log posterior snapshots and likelihoods  
- Optional reward shaping from posterior confidence  

### 5. **Curriculum Learning**
- CurriculumManager chooses savestates  
- Promote/demote checkpoints based on success  
- Integrates with vectorized resets  

---

# **ğŸ“Š Logging Requirements (`logging_utils.py`)**

Log the following:

### **Performance**
- Steps/sec (SPS)
- Total env fps
- Episodes per hour
- Parallelization stats

### **Flags + Quests**
- WRAM flag transitions
- Flag likelihood distributions
- Bayesian posterior over quests/cities/stages

### **Model Internals**
- RND prediction error mean/stdev
- Novelty reward contributions
- Q-value entropy

---

# **ğŸ¥ Video Requirements (`video_utils.py`)**

- Capture episode rollouts  
- Render frames â†’ save to mp4 or gif  
- Optionally overlay:
  - Episode reward  
  - WRAM flag indicator text  
  - Quest posterior summary  

---

# **ğŸ§ª Debug Scripts**

Add:

```
Daddy/debug_rollout.py
```

Must verify:
- Env â†’ model integration  
- WRAM flag decoding  
- RND output finite  
- Steps/sec normal  
- Video capture works  

---

# **ğŸ› ï¸ Implementation Order for Copilot**
1. Scaffold folder structure  
2. Implement networks.py (CNN + GRU + Q-head)  
3. Implement rnd.py  
4. Implement flags.py  
5. Implement bayes_quests.py  
6. Implement curriculum.py  
7. Implement replay_buffer.py  
8. Implement logging_utils.py  
9. Implement video_utils.py  
10. Implement agent.py (combining everything)  
11. Implement train_slim.py  

---

# **ğŸ’¬ Usage with Copilot**

Inside VSCode:
- Open a file in `Daddy/`
- Tell Copilot:
  > â€œUse `Daddy/DESIGN_SLIM_MODEL.md` and scaffold the files exactly as specified.â€

All future work happens inside `Daddy/` only.

---

