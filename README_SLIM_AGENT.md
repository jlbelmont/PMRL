# Slim PokÃ©mon Red RL Agent â€” GitHub Repository README

# ğŸ® **Slim PokÃ©mon Red RL Agent (DQN + WRAM Flags + RND + Curriculum)**  
### **A clean, modular replacement for `Final/epsilon/`, fully compatible with `pokemonred_puffer`**

---

## â­ Overview

This repository implements a **slim, efficient Deep Q-Network agent** for PokÃ©mon Red, designed to run on the canonical `drubinstein/pokemonred_puffer` environment without modifying it.

Your old model lives in:

```
Final/epsilon/
```

This new system lives entirely in:

```
Daddy/
```

Both can coexist without interference.

---

# ğŸ“ **Repository Structure**

```
.
â”œâ”€â”€ pokemonred_puffer/            # Do NOT modify (official env)
â”œâ”€â”€ Final/
â”‚   â””â”€â”€ epsilon/                  # Old model (leave untouched)
â”œâ”€â”€ Daddy/                        # New slim agent (this project)
â”‚   â”œâ”€â”€ DESIGN_SLIM_MODEL.md
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ networks.py
â”‚   â”œâ”€â”€ rnd.py
â”‚   â”œâ”€â”€ flags.py
â”‚   â”œâ”€â”€ bayes_quests.py
â”‚   â”œâ”€â”€ curriculum.py
â”‚   â”œâ”€â”€ replay_buffer.py
â”‚   â”œâ”€â”€ logging_utils.py
â”‚   â”œâ”€â”€ video_utils.py
â”‚   â”œâ”€â”€ train_slim.py
â”‚   â”œâ”€â”€ debug_rollout.py
â””â”€â”€ ...
```

---

# ğŸš€ **Project Goals**

The slim agent must:

### âœ”ï¸ Stay fully compatible with `pokemonred_puffer`  
### âœ”ï¸ Not modify `Final/epsilon/`  
### âœ”ï¸ Implement a clean DQN architecture  
### âœ”ï¸ Use WRAM flags as objectives + state features  
### âœ”ï¸ Support vectorized envs (multi-env swarming)  
### âœ”ï¸ Include:
- Random Network Distillation  
- Episodic novelty reward  
- Bayesian quest/flag posterior model  
- Swarmed curriculum learning  
- Comprehensive logging (steps/sec, flag likelihoods, posterior dumps)  
- Video recording of rollouts  

---

# ğŸ”§ **SlimDQN Architecture Requirements**

### Inputs:
- 72Ã—80 grayscale frame stacks (4â€“8 frames)
- Structured map/goal features
- **WRAM flag embeddings**

### Core architecture:
- **Slim CNN** (2â€“3 simple conv layers)
- **GRU or LSTM (128 hidden)**
- **Linear Q-value head**

Total params target: **2â€“6 million**.

---

# âš™ï¸ Training Loop (train_slim.py)

### Must include:
- Vectorized PokÃ©mon Red envs  
- Replay buffer  
- Target network  
- Îµ-greedy schedule  
- RND intrinsic reward  
- Episodic novelty bonus  
- Curriculum learning  
- Bayesian quest/flag posterior  
- Logging + video recording  

---

# ğŸ“Š Logging Requirements

- Steps/sec (SPS)
- Env parallelization stats
- RND prediction error distribution
- Novelty reward logs
- WRAM flag likelihoods
- Bayesian posterior over quests/cities
- Q-value entropy
- Episode reward summaries

---

# ğŸ¥ Video Requirements

- Capture rollouts to MP4 or GIF  
- Optional overlays:
  - Episode reward  
  - WRAM flag states  
  - Quest posterior snapshot  

---

# ğŸ§ª Debug Script

`debug_rollout.py` must:
- Run a brief vectorized rollout  
- Log steps/sec  
- Verify WRAM flag decoding  
- Verify RND works (no NaNs)  
- Optionally save a short video  

---

# ğŸ› ï¸ How to Use This With Copilot/Codex

Open VSCode â†’ open a file in `Daddy/` â†’ open Copilot Chat â†’ send:

```
Read Daddy/DESIGN_SLIM_MODEL.md and treat it as the specification.
Follow it exactly and scaffold the new slim agent inside Daddy/.
Do NOT modify pokemonred_puffer/ or Final/epsilon/.
Start by generating networks.py and agent.py.
```

Then follow up:

```
Now create rnd.py exactly as described.
```

```
Now create flags.py exactly as described.
```

etcâ€¦ until all files match the spec.

---

# âœ”ï¸ This repo is now ready for GitHub

Just commit:

```
git add .
git commit -m "Add slim PokÃ©mon Red RL agent architecture and design spec"
git push
```

Enjoy building your slim RL system!
